{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Download the data\n",
        "Let's download and uncompress our data and images here:"
      ],
      "metadata": {
        "id": "dhTEdM56b3-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "display(platform.system())\n",
        "import os\n",
        "file_download_link = 'https://www.dropbox.com/scl/fi/x4vhkglosags3qmg4h0p2/hw3data.zip?rlkey=kke6onzuc2rajohgislutjgg7&dl=0'\n",
        "if os.name == 'nt':\n",
        "    print('Please download your dataset here:', file_download_link)\n",
        "else:\n",
        "    # We need to first download the data here:\n",
        "    !wget -O data.zip \"$file_download_link\" -o /dev/null\n",
        "    !unzip data.zip > /dev/null"
      ],
      "metadata": {
        "id": "gMPNGHDXbxEw",
        "outputId": "31b5fad3-cf07-4173-ccfa-32cc3eb59952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Linux'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If your data is on google drive then uncomment the code below to access\n",
        "# your google drive.\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k8pzndEXoGoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Tensorflow Keras on our Titanic dataset (25 points)\n",
        "[tf.keras.models](https://www.tensorflow.org/api_docs/python/tf/keras/Model), [tf.keras.layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)\n",
        "### Q1: We will now implement customization via Keras. Be creative building you NN.\n",
        "Make sure you set the verbose parameter to 0 when you train your model. Not doing so will result in your TA's being unable to grade your submission.\n",
        "You can use history to plot your Loss/Metrics.\n",
        "Make sure you generate a Loss/Metrics plot for each question."
      ],
      "metadata": {
        "id": "PUrkSv1vSaRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prerequisite library imports\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Let's reimport our data\n",
        "df = pd.read_csv('./data/titanic/train_data.csv')\n",
        "X = df.drop(['Unnamed: 0', 'PassengerId'], axis=1)\n",
        "y = df['Survived']\n",
        "X = X.drop(columns = 'Survived')"
      ],
      "metadata": {
        "id": "qwAdEE9sSwUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVC85pIxYnae"
      },
      "source": [
        "### 1.1) Based on the imports above we will use those keras libraries to build our models. Here we want to implement a form of scaling to your data either minmax normalization or standardization using the sklearn.preprocessing libraries. Justify why you chose one over the other. Is this classfication or regression? (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_I8JzfHvOSQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please use your scalarization of X here: then run the cell below to split your training and test data.\n",
        "# Scalarization means normalizing or standardizing\n"
      ],
      "metadata": {
        "id": "c4f1E7_QnuJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
        "display(X_train.shape)\n",
        "display(y_train.shape)"
      ],
      "metadata": {
        "id": "xKhAcFmxs5es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your model, and training here\n",
        "model = Sequential()"
      ],
      "metadata": {
        "id": "jIHVwzIYSZjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now lets compile our model using the function compile\n",
        "### Here we will use rmsprop as an optimizer and binary crossentropy as our loss function"
      ],
      "metadata": {
        "id": "mnxn0OJvKbxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy')"
      ],
      "metadata": {
        "id": "kuLxxVo9KQrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ru-Sg5jYnae"
      },
      "source": [
        "### 1.2) Using the example for traindata above create a model using different activation functions by setting MYACTIVATIONFXN: (10 points)\n",
        "\n",
        "Here is the example code you can use to build your own DNN after you check the shape of your X matrix. Similar to HW2\n",
        "```python\n",
        "# Hint! You can start with model.add(Dense(units = 16, activation = 'relu', input_dim = ?))\n",
        "# Make sure the input_dim parameter is set to the number of features in your X matrix.\n",
        "MYACTIVATIONFXN = 'SOMEFXN'\n",
        "model.add(Dense(units = 14, activation = MYACTIVATIONFXN, input_dim = ?))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's initialize our model\n",
        "model = Sequential() # Initialising the ANN/DNN"
      ],
      "metadata": {
        "id": "mPxlO3Xwtayb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Check the shape of our data!\n",
        "# This should match your input layer\n",
        "X.shape"
      ],
      "metadata": {
        "id": "SHWjAD8Otzbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6D4qvTGYnaf"
      },
      "outputs": [],
      "source": [
        "# If you decide to initially use a sigmoid, make sure the number of units matches the number of targets\n",
        "# in this case we only have 1 target so for sigmoid you need to set units to 1\n",
        "# Please use the example code above in the hint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now lets compile our model using the function compile\n",
        "### Here we will use rmsprop as an optimizer and binary crossentropy as our loss function"
      ],
      "metadata": {
        "id": "yfInuSI-UH0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy')\n"
      ],
      "metadata": {
        "id": "FPF064_rUIGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement tensorflows [early stopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) library. Feel free to play with the settings and parameters"
      ],
      "metadata": {
        "id": "kJEr9VGaQjxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0,\n",
        "    patience=0,\n",
        "    verbose=0,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    start_from_epoch=0\n",
        ")"
      ],
      "metadata": {
        "id": "yfEfQQlfQjAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here we will run our ANN/DNN using the fit function using a batch size of 1 and 10 epochs\n",
        "Early stopping has been added to your model.fit call"
      ],
      "metadata": {
        "id": "8xXvT24DUlRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I have provided the code for you here:\n",
        "# Feel free to play around with the code as you please\n",
        "history = model.fit(X_train.astype('float'), y_train, batch_size = 1, epochs = 10, callbacks = [early_stopping], verbose = 0)\n"
      ],
      "metadata": {
        "id": "ELg4LXeYUpdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AOBK9hbYnaf"
      },
      "source": [
        "### 1.3) How does the error (in terms of accuracy, precision or recall) differ between your models from the [ANN Notebook](https://colab.research.google.com/drive/15iqjgmQje208R-40Gaa7j-GuMu8WzQiQ?usp=sharing#scrollTo=N-xm3KnxYnaW)? Write in one paragraph or less how the error differs and why. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP8EYlBUYnaf"
      },
      "outputs": [],
      "source": [
        "# Hint! Use the predict function if you don't have logits you will need to threshold your results. 0.5 is reasonable.\n",
        "# Please see the BCC jupyter notebook to see how to do this\n",
        "# Predict your train, test\n",
        "# Evaluate your history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cH4ltuutPpgr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfKWyWNHYnaf"
      },
      "source": [
        "## 2) Complex fit of flowers (30 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_PTyVq4Ynaf"
      },
      "source": [
        "The cool stuff starts with more complex functions. The [Deep learning course from Andrew Ng](https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning) show a way to predict [Rose-functions](https://en.wikipedia.org/wiki/Rose_(mathematics)) using a model with multiple nodes. Lets try that as well! This is similar to our example on tf playground.\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "First we need to import the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMMal0QFYnaf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "data = np.load('./data/rose/rose.npz')\n",
        "X, Y = data['X'], data['Y']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo5iuacCYnaf"
      },
      "source": [
        "To give a feel how it looks, we will first plot the rose, which has 7 petals:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od7DDx7YYnaf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def testModelKeras(X, y, model, h=0.1, f=1.05):\n",
        "    r = X.max()\n",
        "    xmesh, ymesh = np.meshgrid(np.arange(-r*f, r*f+h, h), np.arange(-r*f, r*f+h, h))\n",
        "    Z = model.predict(((np.c_[xmesh.ravel(), ymesh.ravel()])))\n",
        "    Z = (Z > 0.5) * 1\n",
        "    Z = Z.T.reshape(xmesh.shape)\n",
        "    plt.contourf(xmesh, ymesh, Z, cmap=plt.cm.OrRd)\n",
        "    plt.scatter(X[:,0], X[:,1], c=y.flatten().T, cmap=plt.cm.OrRd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuZmpnJAYnag"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "plt.scatter(X[0,:], X[1,:], c=Y.flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Is this classfication or regression? Enter your answer below and why."
      ],
      "metadata": {
        "id": "bIO_ebx0OH-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YwZQztEbOLHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2: We will now implement customization via TensorFlow Keras"
      ],
      "metadata": {
        "id": "nJYtVl06a1Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data = np.load('./data/rose/rose.npz')\n",
        "X, y = data['X'].transpose(), data['Y'].transpose()\n",
        "display(X.shape)\n",
        "display(y.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
        "display(X_train.shape)\n",
        "display(y_train.shape)\n",
        "\n",
        "# Let's initialize our model\n",
        "model = Sequential() # Initialising the ANN"
      ],
      "metadata": {
        "id": "cZF8N-qpbJ-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaSeK8fpYnah"
      },
      "source": [
        "### 2.1) Using the example above, try different number of nodes(units) and different activation functions. How does your loss change? (10 points)\n",
        "Use history to extract the history of your metrics and loss\n",
        "Enable call backs as you did in Q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUa1-L50Ynah"
      },
      "outputs": [],
      "source": [
        "# build your model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile your model"
      ],
      "metadata": {
        "id": "NL_ieFi1RS_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up your early stopping call backs"
      ],
      "metadata": {
        "id": "opy06KYbRW_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train.astype('float'), y_train, batch_size = 10, epochs = 10, verbose = 0)"
      ],
      "metadata": {
        "id": "YdwIYLZ2P3HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVKN7RMLYnah"
      },
      "source": [
        "### 2.2) Using 2.1 as a reference, create two new models and calculate your new error using classification report. Also, using the metrics, explain why you see the same or why you see a different error. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvi8QHpjYnah"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvcIw3pHYnah"
      },
      "source": [
        "### 2.3) Choose your best model! Now plot the new results using the plotting example shown above but using our newly trained best/coolest model. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK14EFkPYnai"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRRH5UW1Ynai"
      },
      "source": [
        "## 3) Cats vs not cats (40 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3: Let's find some cute kittens!"
      ],
      "metadata": {
        "id": "KNymHxiNiY4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data = np.load('./data/cats/cats.npz')\n",
        "X_train, y_train = data['Xtrain'].transpose(), data['Ytrain'].transpose()\n",
        "X_test, y_test = data['Xtest'].transpose(), data['Ytest'].transpose()\n",
        "display(X_train.shape)\n",
        "display(y_train.shape)\n",
        "\n",
        "# Let's initialize our model\n",
        "model = Sequential() # Initialising the ANN"
      ],
      "metadata": {
        "id": "UVrE_ZFuiarQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrCvQs6NYnak"
      },
      "source": [
        "### 3.1) Same as before, build a new model with different number of hidden layers, nodes and activation functions. Describe reason for any similarity or difference (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmdATv6tYnak"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense# Try using different iterations using a simple layout like above. 10, 100, 1000 epochs.\n",
        "# What happens with your loss?\n",
        "# I have written the basics of the code for you\n",
        "MYACTIVATIONFXN = 'relu'\n",
        "model.add(Dense(units = 128, activation = MYACTIVATIONFXN, input_dim = X_train.shape[1]))\n",
        "model.add(Dense(units = 64, activation = MYACTIVATIONFXN))\n",
        "model.add(Dense(units = 32, activation = MYACTIVATIONFXN))\n",
        "model.add(Dense(units = 16, activation = MYACTIVATIONFXN))\n",
        "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement early stopping and [model checkpointing](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to save your model weights. experiment with other call backs to get your best validation metric. For callbacks, you can save your weights and set up a monitor"
      ],
      "metadata": {
        "id": "WM4YcSOcRjEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    #Enter your parameters\n",
        "\n",
        ")\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    #Enter your paramaters\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "VGUrbX9ysgJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's fit our data!"
      ],
      "metadata": {
        "id": "w6PLcpRxRxAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train.astype('float'), y_train, batch_size = 10, epochs = 100, callbacks = [early_stopping, model_checkpoint], verbose = 0)"
      ],
      "metadata": {
        "id": "i8ZnyJyiRxMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Try using different layers and activation function with different number of nodes\n",
        "What happens when you add convolutional layers? What happens to our training loss? </br>\n",
        "After intitializing your model, check if you need to rescale your data. If you do, make sure you [rescale](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling) using: ``` keras.layers.Rescaling(1./255) ``` </br>\n",
        "If you do not, explain how you know that rescaling is not necessary. </br>\n",
        "Here you will begin to add convolutional layers [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) as well as [max pooling 2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPooling2D). You typically want to do max pooling when you change the shape of your conv2d. Max pooling will focus on the most informative features and reduce the memory footprint </br>\n",
        "This also requires reshaping form 1D to 2D. Hint: Look at the plotting fxn\n",
        "```\n",
        "model.add(Conv2D(32, kernel_size=3, activation='leakyrelu', input_shape=(64, 64, 3)))\n",
        "model.add(MaxPooling2D())\n",
        "```\n",
        "Make sure you [flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) before going back into 1D\n",
        "Make sure your ouput layer performs a binary output for a class kitten and class not kitten\n",
        "```\n",
        "model.add(Flatten())\n",
        "```\n",
        "After you flatten, you can add your dense layers once again. </br></br>\n",
        "<em> Note: As noted above, you will have to convert your 1D array back into a 2D array prior to running your convolutional NN. Hint: Look at your plotting function down below!!! </em>"
      ],
      "metadata": {
        "id": "fpPlSApU0ea6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dVYjQT00stLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction step. Make sure you use ```yhat_train``` and ```yhat_test``` variable names for your predictions!"
      ],
      "metadata": {
        "id": "4DMjtOg5BJEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_train = [0]*50 # change me\n",
        "yhat_test = [0]*50 # change me"
      ],
      "metadata": {
        "id": "quVIixdFBRVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmp8LC30Ynak"
      },
      "source": [
        "### 3.2) Calculate your accuracy (10 points)\n",
        "Here you will use both your classification report and your confusion matrix. </br>\n",
        "Later you will be asked to calculate values manually. You are welcome to pull values from your reports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7xQi21-Ynak"
      },
      "outputs": [],
      "source": [
        "# Hint! Use the predict function and threshold your results. 0.5 is reasonable.\n",
        "# In your classification report since we are only predicting cats you will need to set the parameter labels\n",
        "# labels=np.unique(yhat_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf_Khwq1Ynak"
      },
      "source": [
        "### 3.3) Calculate your precision and recall manually as done in SA1. You cannot use values from your classification report or confusion matrix (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8BpIv03Ynal"
      },
      "outputs": [],
      "source": [
        "# Recall calculation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision calculation"
      ],
      "metadata": {
        "id": "BriMkolotghk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's plot!!!"
      ],
      "metadata": {
        "id": "n3E_cvZbkb_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3 # number of images to print\n",
        "imgs = X_test.reshape([50, 64, 64, 3]) # here we reshape our images so that they are 2D again\n",
        "fig, ax = plt.subplots(1, n, figsize=(16,8))\n",
        "for ix in range(n):\n",
        "    num = np.random.randint(imgs.shape[0]) # randomly selects from 51 images\n",
        "    ax[ix].imshow(imgs[num])\n",
        "    if yhat_test[num] == 0:\n",
        "        ax[ix].set_title('This is clearly not a cat')\n",
        "    else:\n",
        "        ax[ix].set_title('Yup! it\\'s a cat')"
      ],
      "metadata": {
        "id": "rq71-Hdnke1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0ja7ns_Ynan"
      },
      "source": [
        "## 4) Collaborative Statement (5 points)\n",
        "#### You must fill this out even if you worked alone to get credit.\n",
        "\n",
        "It is mandatory to include a Statement of Collaboration in each submission, that follows the guidelines below.\n",
        "Include the names of everyone involved in the discussions (especially in-person ones), and what was discussed.\n",
        "All students are required to follow the academic honesty guidelines posted on the course website. For\n",
        "programming assignments in particular, I encourage students to organize (perhaps using Piazza) to discuss the\n",
        "task descriptions, requirements, possible bugs in the support code, and the relevant technical content before they\n",
        "start working on it. However, you should not discuss the specific solutions, and as a guiding principle, you are\n",
        "not allowed to take anything written or drawn away from these discussions (no photographs of the blackboard,\n",
        "written notes, referring to Piazza, etc.). Especially after you have started working on the assignment, try to restrict\n",
        "the discussion to Piazza as much as possible, so that there is no doubt as to the extent of your collaboration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC4SnF_7Ynal"
      },
      "source": [
        "## Round up!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba-yJKQIYnal"
      },
      "source": [
        "I hope you all had fun, writing your own ANN. In my opinon, writing these things from the ground up is the best way to learn how it actually works. I hope that you see that these systems are not magical, but simple matrix multiplications, unfortunately just a very lot of them. The most difficult part is of course the back propagation, where we need to calculate the gradients. Our simple ANNs are quite doable, but adding more different layers to them, can make it a bit more cumbersome. Still the essence is very similar to what we have done today.\n",
        "\n",
        "My suggestion is to play around with these structures, rewrite parts of them, or even better, write your own from scratch!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJkBdpHDYnal"
      },
      "source": [
        "Please let me know if you have any comments!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo5nI67mYnal"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUZ99lNyYnal"
      },
      "source": [
        "## Apendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc0AoxY_Ynal"
      },
      "source": [
        "### Generating Rose Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNcaQgjQYnal"
      },
      "outputs": [],
      "source": [
        "def generateRoseData():\n",
        "    k=7\n",
        "    pointPerPetal = 100\n",
        "    cutOff = 0.1\n",
        "    r = 4\n",
        "\n",
        "    theta = np.linspace(0,np.pi, pointPerPetal * k)\n",
        "    xx = r * np.cos(k * theta) * np.cos(theta)\n",
        "    yy = r * np.cos(k * theta) * np.sin(theta)\n",
        "    cc = [np.ones(pointPerPetal) if ix % 3 == 0 else np.zeros(pointPerPetal) for ix in np.arange(k)]\n",
        "    cc = np.roll(np.hstack(cc).astype(np.uint8), -pointPerPetal//2)\n",
        "    x = xx[(xx**2 + yy**2)**0.5 > cutOff]\n",
        "    y = yy[(xx**2 + yy**2)**0.5 > cutOff]\n",
        "    col = cc[(xx**2 + yy**2)**0.5 > cutOff]\n",
        "    X = np.vstack([x,y])\n",
        "    Y = np.copy(col).reshape([1, -1])\n",
        "    return X, Y\n",
        "X, Y = generateRoseData()\n",
        "np.savez_compressed('./data/rose/rose.npz', X=X, Y=Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq4YAjRgYnam"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBlwWwKQYnam"
      },
      "source": [
        "### Processing Andrews CatvNotCat data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lkHWbzVYnam"
      },
      "outputs": [],
      "source": [
        "# If you get an error here, install h5py via pip3 install h5py\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUyH9EvUYnam"
      },
      "outputs": [],
      "source": [
        "# Data downloaded from:\n",
        "# https://github.com/ridhimagarg/Cat-vs-Non-cat-Deep-learning-implementation\n",
        "def processCatData():\n",
        "    train_dataset = h5py.File(\"./data/cats/train_catvnoncat.h5\", mode='r')\n",
        "    Xtrain = np.array(train_dataset[\"train_set_x\"])\n",
        "    Y_train = np.array(train_dataset[\"train_set_y\"])\n",
        "    test_dataset = h5py.File(\"./data/cats/test_catvnoncat.h5\", mode='r')\n",
        "    Xtest = np.array(test_dataset[\"test_set_x\"])\n",
        "    Y_test = np.array(test_dataset[\"test_set_y\"])\n",
        "    X_train = Xtrain / 255\n",
        "    X_test = Xtest / 255\n",
        "    X_train = X_train.reshape(209, -1).T\n",
        "    Y_train = Y_train.reshape(-1, 209)\n",
        "    X_test = X_test.reshape(50, -1).T\n",
        "    Y_test = Y_test.reshape(-1, 50)\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "Xtrain, Xtest, Ytrain, Ytest = processCatData()\n",
        "np.savez_compressed('./data/cats/cats.npz', Xtrain=Xtrain, Xtest=Xtest, Ytrain=Ytrain, Ytest=Ytest)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cofYYytwRk5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy2XhQSoYnan"
      },
      "source": [
        "# Credits\n",
        "#### Edwin Solares - Updates to Part 1, Conversion to google colab, conversion to Keras and preprocessing data to work with Kears (Part 2).\n",
        "#### Dennis Bakhuis - Custom ANN class and it's example exercises (Part 1). May the Fourth (be with you) 2020\n",
        "https://linkedin.com/in/dennisbakhuis/ \\\n",
        "https://github.com/dennisbakhuis\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}